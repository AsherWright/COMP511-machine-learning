{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMP 551 - Assignment 2\n",
    "# Asher Wright - 260559393\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Q1: Creating our dataset\n",
    "\n",
    "#read data from files\n",
    "ds1_mean_neg = np.genfromtxt(open(\"Datasets/DS1_m_0.txt\", \"rb\"), delimiter=\",\")\n",
    "ds1_mean_pos = np.genfromtxt(open(\"Datasets/DS1_m_1.txt\", \"rb\"), delimiter=\",\")\n",
    "#remove all nan values (since extra comma at ends)\n",
    "ds1_mean_neg = ds1_mean_neg[~np.isnan(ds1_mean_neg)]\n",
    "ds1_mean_pos = ds1_mean_pos[~np.isnan(ds1_mean_pos)]\n",
    "#generate and read in covariance matrix\n",
    "ds1_cov = np.zeros((len(ds1_mean_neg), len(ds1_mean_neg)))\n",
    "\n",
    "cov_count = 0\n",
    "with open(\"Datasets/DS1_Cov.txt\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\",\")[:-1] #remove '\\n'\n",
    "        ds1_cov[cov_count] = map(np.float, values)        \n",
    "        cov_count +=1\n",
    "\n",
    "#generate the 2000 examples for each, some test and some train\n",
    "pos_test_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=600)\n",
    "neg_test_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=600)\n",
    "pos_train_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=1400)\n",
    "neg_train_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=1400)\n",
    "\n",
    "#TODO: figure out if they want four separate files (like in A1) or one file with labels inside\n",
    "np.savetxt(\"Datasets/DS1_positive_testing\", pos_test_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS1_positive_training\", pos_train_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS1_negative_testing\", neg_test_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS1_negative_training\", neg_train_examples, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2:\n",
    "def calculate_LDA_params(pos_train, neg_train):\n",
    "    #The parameters to estimate are pi, u1, u2, and sigma.\n",
    "    N1 = len(pos_train)\n",
    "    N2 = len(neg_train)\n",
    "    #Max likelihood says pi = N1/(N1 + N2)\n",
    "    pi = float(N1) / float(N1 + N2)\n",
    "    #Max likelihood says ui = mean of all points of type i\n",
    "    mu_1 = np.mean(pos_train, 0)\n",
    "    mu_2 = np.mean(neg_train, 0)\n",
    "    #Max likelihood says sigma = N1/N * S1 + N2/N * S2 where Si = 1/Ni* SUM[(X(n)-ui)(X(n)-ui)^T]\n",
    "\n",
    "    #calculate S1 and S2 using the training examples\n",
    "    S1 = np.cov(pos_train.T)\n",
    "    S2 = np.cov(neg_train.T)\n",
    "    #not really necessary since we know it's equal (and half and half)\n",
    "    Sigma = (N1*S1)/(N1 + N2) + (N2 * S2)/(N1 + N2)\n",
    "\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    #now calculate the parameters of the model\n",
    "    w_param = np.matmul(Sigma_inv,(mu_1 - mu_2))\n",
    "    #print(Sigma_inv)\n",
    "    w_0_param = (-1)* np.matmul(np.matmul(mu_1.T,Sigma_inv), mu_1) / 2. + np.matmul(np.matmul(mu_2.T, Sigma_inv), mu_2) / 2. + np.log(pi/(1 - pi))\n",
    "    return w_param, w_0_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9475\n"
     ]
    }
   ],
   "source": [
    "#We now have our w and w_0 from the training set. We will use it on the test set.\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def LDA_accuracy(w, w_0, pos_test, neg_test):\n",
    "    wrongly_classified = 0.\n",
    "    for val in pos_test:\n",
    "        probability = sigmoid(np.matmul(w.T, val)+w_0)\n",
    "        if(probability < 0.5):\n",
    "            wrongly_classified +=1\n",
    "\n",
    "    for val in neg_test:\n",
    "        probability = sigmoid(np.matmul(w.T, val)+w_0)\n",
    "        if(probability > 0.5):\n",
    "            wrongly_classified +=1\n",
    "\n",
    "    return (len(pos_test) + len(neg_test) - wrongly_classified)/(len(pos_test) + len(neg_test))\n",
    "\n",
    "w, w_0 = calculate_LDA_params(pos_train_examples, neg_train_examples)\n",
    "print(LDA_accuracy(w, w_0, pos_test_examples, neg_test_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: K-NN\n",
    "# go through the test set\n",
    "# for each one, find the k nearest neighbours in the train set\n",
    "# depending on whether there are more positive or negative, label the point as such.\n",
    "# we choose k odd so that we don't have any ties.\n",
    "def k_NN(pos_train, neg_train, pos_test, neg_test):\n",
    "    all_train_examples = np.concatenate((pos_train, neg_train))\n",
    "    accuracies = []\n",
    "    ks = xrange(1, 99, 2)\n",
    "    for k in ks:\n",
    "        wrongly_classified = 0.\n",
    "\n",
    "        #do our positive test examples first:\n",
    "        for val in pos_test:\n",
    "            #take the difference\n",
    "            diff = all_train_examples - val\n",
    "            norms = np.linalg.norm(diff, axis=1)\n",
    "            lowest_indices = norms.argsort()[:k]\n",
    "            pos_neighbours = 0.\n",
    "            for ind in lowest_indices:\n",
    "                if(ind < len(pos_train)):\n",
    "                    pos_neighbours +=1.\n",
    "            if pos_neighbours < float(k)/2:\n",
    "                wrongly_classified +=1\n",
    "\n",
    "        #print((len(pos_test_examples) - wrongly_classified)/(len(pos_test_examples)))\n",
    "\n",
    "        neg_wrong_class = 0.\n",
    "        #Now do the same thing for the negative test examples\n",
    "        #for val in neg_test_examples:\n",
    "        for val in neg_test:\n",
    "            #take the difference\n",
    "            diff = all_train_examples - val\n",
    "            norms = np.linalg.norm(diff, axis=1)\n",
    "            lowest_indices = norms.argsort()[:k]\n",
    "            pos_neighbours = 0.\n",
    "            for ind in lowest_indices:\n",
    "                if(ind < len(pos_train)):\n",
    "                    pos_neighbours +=1.\n",
    "            if pos_neighbours > float(k)/2:\n",
    "                wrongly_classified +=1\n",
    "                neg_wrong_class +=1\n",
    "        # now calculate the accuracy for this k, and store it\n",
    "        accuracies.append((len(pos_test) + len(neg_test) - wrongly_classified)/(len(pos_test) + len(neg_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy occurs when k = 145, and is 57.8333333333%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1042f5cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ks, accuracies, 'ro')\n",
    "max_accuracy = max(accuracies)\n",
    "max_k = accuracies.index(max_accuracy)\n",
    "print(\"The highest accuracy occurs when k = \" + str(ks[max_k]) + \", and is \" + str(100*max_accuracy) + \"%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4 reading in data\n",
    "ds2_mean_pos_1 = np.genfromtxt(open(\"Datasets/DS2_c1_m1.txt\", \"rb\"), delimiter=\",\")\n",
    "ds2_mean_pos_2 = np.genfromtxt(open(\"Datasets/DS2_c1_m2.txt\", \"rb\"), delimiter=\",\")\n",
    "ds2_mean_pos_3 = np.genfromtxt(open(\"Datasets/DS2_c1_m3.txt\", \"rb\"), delimiter=\",\")\n",
    "ds2_mean_neg_1 = np.genfromtxt(open(\"Datasets/DS2_c2_m1.txt\", \"rb\"), delimiter=\",\")\n",
    "ds2_mean_neg_2 = np.genfromtxt(open(\"Datasets/DS2_c2_m2.txt\", \"rb\"), delimiter=\",\")\n",
    "ds2_mean_neg_3 = np.genfromtxt(open(\"Datasets/DS2_c2_m3.txt\", \"rb\"), delimiter=\",\")\n",
    "\n",
    "# Remove nan values\n",
    "ds2_mean_pos_1 = ds2_mean_pos_1[~np.isnan(ds2_mean_pos_1)]\n",
    "ds2_mean_pos_2 = ds2_mean_pos_2[~np.isnan(ds2_mean_pos_2)]\n",
    "ds2_mean_pos_3 = ds2_mean_pos_3[~np.isnan(ds2_mean_pos_3)]\n",
    "ds2_mean_neg_1 = ds2_mean_neg_1[~np.isnan(ds2_mean_neg_1)]\n",
    "ds2_mean_neg_2 = ds2_mean_neg_2[~np.isnan(ds2_mean_neg_2)]\n",
    "ds2_mean_neg_3 = ds2_mean_neg_3[~np.isnan(ds2_mean_neg_3)]\n",
    "\n",
    "#Read in all of the covariance matrices\n",
    "ds2_cov1 = np.zeros((len(ds2_mean_pos_1), len(ds2_mean_pos_1)))\n",
    "ds2_cov2 = np.zeros((len(ds2_mean_pos_2), len(ds2_mean_pos_2)))\n",
    "ds2_cov3 = np.zeros((len(ds2_mean_pos_3), len(ds2_mean_pos_3)))\n",
    "\n",
    "cov_count = 0\n",
    "with open(\"Datasets/DS2_Cov1.txt\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\",\")[:-1] #remove '\\n'\n",
    "        ds2_cov1[cov_count] = map(np.float, values)        \n",
    "        cov_count +=1\n",
    "    \n",
    "cov_count = 0\n",
    "with open(\"Datasets/DS2_Cov2.txt\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\",\")[:-1] #remove '\\n'\n",
    "        ds2_cov2[cov_count] = map(np.float, values)        \n",
    "        cov_count +=1\n",
    "\n",
    "cov_count = 0\n",
    "with open(\"Datasets/DS2_Cov3.txt\", \"rb\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(\",\")[:-1] #remove '\\n'\n",
    "        ds2_cov3[cov_count] = map(np.float, values)        \n",
    "        cov_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now that we've read in all our data we can generate our data set\n",
    "# We will have 1200 test values and 2800 training values (same as before)\n",
    "# Based on the probabilities: 0.1, 0.42, 0.48, we want [120 test & 280 train, 504 test & 1176 train, 576 & 1344 train]\n",
    "# We are also going for half of each to be positive and half to be negative\n",
    "pos_test_1_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=60)\n",
    "neg_test_1_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=60)\n",
    "pos_train_1_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=140)\n",
    "neg_train_1_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=140)\n",
    "pos_test_2_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=252)\n",
    "neg_test_2_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=252)\n",
    "pos_train_2_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=588)\n",
    "neg_train_2_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=588)\n",
    "pos_test_3_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=288)\n",
    "neg_test_3_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=288)\n",
    "pos_train_3_examples = np.random.multivariate_normal(ds1_mean_pos, ds1_cov, size=672)\n",
    "neg_train_3_examples = np.random.multivariate_normal(ds1_mean_neg, ds1_cov, size=672)\n",
    "\n",
    "#we now have all of our data in, we can combine it into one data set, with labels, to make it easier to handle.\n",
    "positive_test_examples = np.concatenate((pos_test_1_examples,pos_test_2_examples,pos_test_3_examples))\n",
    "negative_test_examples = np.concatenate((neg_test_1_examples,neg_test_2_examples,neg_test_3_examples))\n",
    "positive_train_examples = np.concatenate((pos_train_1_examples,pos_train_2_examples,pos_train_3_examples))\n",
    "negative_train_examples = np.concatenate((neg_train_1_examples,neg_train_2_examples,neg_train_3_examples))\n",
    "\n",
    "np.savetxt(\"Datasets/DS2_positive_testing\", positive_test_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS2_positive_training\", positive_train_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS2_negative_testing\", negative_test_examples, delimiter=',')\n",
    "np.savetxt(\"Datasets/DS2_negative_training\", negative_train_examples, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955833333333\n"
     ]
    }
   ],
   "source": [
    "# now we can do LDA on the data set\n",
    "w, w_0 = calculate_LDA_params(positive_train_examples, negative_train_examples)\n",
    "print(LDA_accuracy(w, w_0, positive_test_examples, negative_test_examples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
