{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn import tree, svm\n",
    "#data import\n",
    "#taken from Reddit discussion: https://www.reddit.com/r/McGill_comp551s1/comments/7x2sdy/assignment3/\n",
    "yelp_train = pd.read_csv('Datasets/yelp-train.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "yelp_test = pd.read_csv('Datasets/yelp-test.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "yelp_valid = pd.read_csv('Datasets/yelp-valid.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "imdb_train = pd.read_csv('Datasets/IMDB-train.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "imdb_test =  pd.read_csv('Datasets/IMDB-test.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "imdb_valid =  pd.read_csv('Datasets/IMDB-valid.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data clean up\n",
    "def pre_process(review):\n",
    "    #we remove the breaks from the dataset\n",
    "    review = re.sub('<br /><br />', '', review)\n",
    "    return re.sub(r'[^a-zA-Z ]', '', review).lower()\n",
    "\n",
    "#remove all punctuation and put to lower case\n",
    "for i, row in yelp_train.iterrows():\n",
    "    yelp_train.set_value(i,'Comment',pre_process(row['Comment']))\n",
    "for i, row in imdb_train.iterrows():\n",
    "    imdb_train.set_value(i,'Comment',pre_process(row['Comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(training_set):\n",
    "    # Now go through and find the frequency of each word\n",
    "    unsorted_vocabulary = {}\n",
    "\n",
    "    #populate the unsorted vocab\n",
    "    for review in training_set['Comment']:\n",
    "        #filter removes the blank spaces\n",
    "        for word in filter(None, review.split(' ')):\n",
    "            if(word in unsorted_vocabulary):\n",
    "                    unsorted_vocabulary[word] += 1\n",
    "            else:\n",
    "                unsorted_vocabulary[word] = 1\n",
    "\n",
    "    #now we will create a sorted vocab of the top 10000 words\n",
    "    return collections.Counter(unsorted_vocabulary).most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_bbow_vector(words, sentence):\n",
    "    sentence = pre_process(sentence)\n",
    "    vector = np.zeros(10000)\n",
    "    for word in filter(None, sentence.split(' ')):\n",
    "        if(word in words):\n",
    "            vector[words.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "def generate_freq_vector(words, sentence):\n",
    "    sentence = pre_process(sentence)\n",
    "    vector = np.zeros(10000)\n",
    "    total_words_in_vocab = 0\n",
    "    for word in filter(None, sentence.split(' ')):\n",
    "        if(word in words):\n",
    "            total_words_in_vocab += 1.\n",
    "            vector[words.index(word)] += 1.\n",
    "    for i in range(0, len(vector)):\n",
    "        if(total_words_in_vocab != 0):\n",
    "            vector[i] = vector[i] / total_words_in_vocab\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_vocabulary = get_vocabulary(yelp_train)\n",
    "yelp_words = [i[0] for i in yelp_vocabulary]\n",
    "\n",
    "imdb_vocabulary = get_vocabulary(imdb_train)\n",
    "imdb_words = [i[0] for i in imdb_vocabulary]\n",
    "\n",
    "# print(generate_bbow_vector(yelp_words, \"willow and of the\"))\n",
    "# print(generate_freq_vector(yelp_words, \"willow asdlkfjalsdfk and and of the\"))\n",
    "# print(generate_bbow_vector(imdb_words, \"willow and of the\"))\n",
    "# print(generate_freq_vector(imdb_words, \"willow asdlkfjalsdfk and and of the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will create the IMDB and YELP datasets, as described in the report handout\n",
    "def create_vocab_file(vocabulary, filename):\n",
    "    word_count = 1\n",
    "    with open(filename, 'w') as the_file:\n",
    "        for i, item in vocabulary:\n",
    "            the_file.write(str(word_count) + \"\\t\" + str(i) + \"\\t\" + str(item) + \"\\n\")\n",
    "            word_count +=1\n",
    "\n",
    "create_vocab_file(yelp_vocabulary, \"Datasets/yelp-vocab.txt\")\n",
    "create_vocab_file(imdb_vocabulary, \"Datasets/IMDB-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From here on in, we will be actually training and testing\n",
    "\n",
    "# this method will take in a bunch of data and calculate F1\n",
    "def random_classifier(train_data, test_data, min_class, max_class):\n",
    "    test_prediction = []\n",
    "    train_prediction = []\n",
    "    \n",
    "    # go through data and choose a random class\n",
    "    for i in range(0, len(train_data)):\n",
    "        class_predict = random.randint(min_class, max_class)\n",
    "        train_prediction.append(class_predict)\n",
    "    for i in range(0, len(test_data)):\n",
    "        class_predict = random.randint(min_class, max_class)\n",
    "        test_prediction.append(class_predict)\n",
    "    return train_prediction, test_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.197714285714, Test F1 is 0.2055\n"
     ]
    }
   ],
   "source": [
    "#Yelp Random\n",
    "yelp_random_train_predict, yelp_random_test_predict = random_classifier(yelp_train, yelp_test, 1, 5)\n",
    "yelp_rand_train_f1 = f1_score(yelp_train['Evaluation'], yelp_random_train_predict, average='micro')\n",
    "yelp_rand_test_f1 = f1_score(yelp_test['Evaluation'], yelp_random_test_predict, average='micro')\n",
    "\n",
    "print(\"Train f1 is \" + str(yelp_rand_train_f1) + \", Test F1 is \" + str(yelp_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.498966597773, Test F1 is 0.501858587474\n"
     ]
    }
   ],
   "source": [
    "#IMDB Random\n",
    "\n",
    "imdb_random_train_predict, imdb_random_test_predict = random_classifier(imdb_train, imdb_test, 0,1)\n",
    "imdb_rand_train_f1 = f1_score(imdb_train['Evaluation'], imdb_random_train_predict)\n",
    "imdb_rand_test_f1 = f1_score(imdb_test['Evaluation'], imdb_random_test_predict)\n",
    "\n",
    "\n",
    "print(\"Train f1 is \" + str(imdb_rand_train_f1) + \", Test F1 is \" + str(imdb_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_classifier(train_data, test_data):\n",
    "    train_mode = train_data['Evaluation'].mode()[0]\n",
    "    test_mode = test_data['Evaluation'].mode()[0]\n",
    "    \n",
    "    train_prediction = [train_mode] * len(train_data)\n",
    "    test_prediction = [test_mode] * len(test_data)\n",
    "    return train_prediction, test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.352571428571, Test F1 is 0.351\n"
     ]
    }
   ],
   "source": [
    "#Yelp Majority Classifier\n",
    "yelp_maj_train_predict, yelp_maj_test_predict = majority_classifier(yelp_train, yelp_test)\n",
    "yelp_rand_train_f1 = f1_score(yelp_train['Evaluation'], yelp_maj_train_predict, average='micro')\n",
    "yelp_rand_test_f1 = f1_score(yelp_test['Evaluation'], yelp_maj_test_predict, average='micro')\n",
    "\n",
    "print(\"Train f1 is \" + str(yelp_rand_train_f1) + \", Test F1 is \" + str(yelp_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.5, Test accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "#IMDB Majority Classifier\n",
    "imdb_maj_train_predict, imdb_maj_test_predict = majority_classifier(imdb_train, imdb_test)\n",
    "imdb_rand_train_f1 = accuracy_score(imdb_train['Evaluation'], imdb_maj_train_predict)\n",
    "imdb_rand_test_f1 = accuracy_score(imdb_test['Evaluation'], imdb_maj_test_predict)\n",
    "\n",
    "print(\"Train accuracy is \" + str(imdb_rand_train_f1) + \", Test accuracy is \" + str(imdb_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will do naive bayes.\n",
    "yelp_vocabulary = get_vocabulary(yelp_train)\n",
    "yelp_words = [i[0] for i in yelp_vocabulary]\n",
    "\n",
    "#first make the training features matrix\n",
    "training_inputs = [[] for i in range(2)]\n",
    "testing_inputs = [[] for i in range(2)]\n",
    "validation_inputs = [[] for i in range(2)]\n",
    "\n",
    "for sentence in yelp_train['Comment']:\n",
    "    training_inputs[0].append(generate_bbow_vector(yelp_words, sentence))\n",
    "    training_inputs[1].append(generate_freq_vector(yelp_words, sentence))\n",
    "\n",
    "for sentence in yelp_test['Comment']:\n",
    "    testing_inputs[0].append(generate_bbow_vector(yelp_words, sentence))\n",
    "    testing_inputs[1].append(generate_freq_vector(yelp_words, sentence))\n",
    "\n",
    "for sentence in yelp_valid['Comment']:\n",
    "    validation_inputs[0].append(generate_bbow_vector(yelp_words, sentence))\n",
    "    validation_inputs[1].append(generate_bbow_vector(yelp_words, sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classifier(classifier, train_input, train_target, valid_input, valid_target, test_input, test_target):\n",
    "    classifier.fit(train_input, train_target)\n",
    "    train_target_pred = classifier.predict(train_input)\n",
    "    valid_target_pred = classifier.predict(valid_input)\n",
    "    test_target_pred = classifier.predict(test_input)\n",
    "\n",
    "    train_f1 = f1_score(train_target, train_target_pred, average='micro')\n",
    "    valid_f1 = f1_score(valid_target, valid_target_pred, average='micro')\n",
    "    test_f1 = f1_score(test_target, test_target_pred, average='micro')\n",
    "    return train_f1, valid_f1, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(classifiers, data_index):\n",
    "    #run it on each classifier and print the results (BBOW)\n",
    "    for classifier in classifiers:\n",
    "        metrics = run_classifier(classifier[1], training_inputs[data_index], yelp_train['Evaluation'], validation_inputs[data_index], yelp_valid['Evaluation'], testing_inputs[data_index], yelp_test['Evaluation'])\n",
    "        print(str(classifier[0]) + str(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes: (0.59771428571428575, 0.38100000000000001, 0.40949999999999998)\n",
      "Decision Trees: (1.0, 0.34999999999999998, 0.34999999999999998)\n",
      "SVM: (0.998, 0.441, 0.44900000000000001)\n"
     ]
    }
   ],
   "source": [
    "#Create the classifiers we want to use to train and test our data\n",
    "classifiers = []\n",
    "#We use bernoulli here since, with the binary bag of words, the features are binary.\n",
    "classifiers.append((\"Bernoulli Naive Bayes: \", BernoulliNB()))\n",
    "classifiers.append((\"Decision Trees: \", tree.DecisionTreeClassifier()))\n",
    "classifiers.append((\"SVM: \", svm.LinearSVC()))\n",
    "\n",
    "#run the classifiers for BBOW (data_index = 0)\n",
    "run_classifiers(classifiers, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now alter our classifiers list for the frequency representation\n",
    "classifiers[0] = (\"Gaussian Naive Bayes: \", GaussianNB())\n",
    "# Now we rerun with data_index = 1 (Frequency data)\n",
    "run_classifiers(classifiers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#And then run it on each classifier and print the results (Frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
