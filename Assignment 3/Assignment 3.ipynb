{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree, svm\n",
    "#data import\n",
    "#taken from Reddit discussion: https://www.reddit.com/r/McGill_comp551s1/comments/7x2sdy/assignment3/\n",
    "yelp_train = pd.read_csv('Datasets/yelp-train.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "yelp_test = pd.read_csv('Datasets/yelp-test.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "imdb_train = pd.read_csv('Datasets/IMDB-train.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])\n",
    "imdb_test =  pd.read_csv('Datasets/IMDB-test.txt',sep='\\t',names=[\"Comment\", \"Evaluation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean up\n",
    "def pre_process(review):\n",
    "    #we remove the breaks from the dataset\n",
    "    review = re.sub('<br /><br />', '', review)\n",
    "    return re.sub(r'[^a-zA-Z ]', '', review).lower()\n",
    "\n",
    "#remove all punctuation and put to lower case\n",
    "for i, row in yelp_train.iterrows():\n",
    "    yelp_train.set_value(i,'Comment',pre_process(row['Comment']))\n",
    "for i, row in imdb_train.iterrows():\n",
    "    imdb_train.set_value(i,'Comment',pre_process(row['Comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(training_set):\n",
    "    # Now go through and find the frequency of each word\n",
    "    unsorted_vocabulary = {}\n",
    "\n",
    "    #populate the unsorted vocab\n",
    "    for review in training_set['Comment']:\n",
    "        #filter removes the blank spaces\n",
    "        for word in filter(None, review.split(' ')):\n",
    "            if(word in unsorted_vocabulary):\n",
    "                    unsorted_vocabulary[word] += 1\n",
    "            else:\n",
    "                unsorted_vocabulary[word] = 1\n",
    "\n",
    "    #now we will create a sorted vocab of the top 10000 words\n",
    "    return collections.Counter(unsorted_vocabulary).most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bbow_vector(words, sentence):\n",
    "    sentence = pre_process(sentence)\n",
    "    vector = np.zeros(10000)\n",
    "    for word in filter(None, sentence.split(' ')):\n",
    "        if(word in words):\n",
    "            vector[words.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "def generate_freq_vector(words, sentence):\n",
    "    sentence = pre_process(sentence)\n",
    "    vector = np.zeros(10000)\n",
    "    total_words_in_vocab = 0\n",
    "    for word in filter(None, sentence.split(' ')):\n",
    "        if(word in words):\n",
    "            total_words_in_vocab += 1.\n",
    "            vector[words.index(word)] += 1.\n",
    "    for i in range(0, len(vector)):\n",
    "        vector[i] = vector[i] / total_words_in_vocab\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0. ...,  0.  0.  1.]\n",
      "[ 0.2  0.4  0.  ...,  0.   0.   0.2]\n",
      "[ 1.  0.  1. ...,  0.  0.  0.]\n",
      "[ 0.25  0.    0.5  ...,  0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "yelp_vocabulary = get_vocabulary(yelp_train)\n",
    "yelp_words = [i[0] for i in yelp_vocabulary]\n",
    "\n",
    "imdb_vocabulary = get_vocabulary(imdb_train)\n",
    "imdb_words = [i[0] for i in imdb_vocabulary]\n",
    "\n",
    "# print(generate_bbow_vector(yelp_words, \"willow and of the\"))\n",
    "# print(generate_freq_vector(yelp_words, \"willow asdlkfjalsdfk and and of the\"))\n",
    "# print(generate_bbow_vector(imdb_words, \"willow and of the\"))\n",
    "# print(generate_freq_vector(imdb_words, \"willow asdlkfjalsdfk and and of the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create the IMDB and YELP datasets, as described in the report handout\n",
    "def create_vocab_file(vocabulary, filename):\n",
    "    word_count = 1\n",
    "    with open(filename, 'w') as the_file:\n",
    "        for i, item in vocabulary:\n",
    "            the_file.write(str(word_count) + \"\\t\" + str(i) + \"\\t\" + str(item) + \"\\n\")\n",
    "            word_count +=1\n",
    "\n",
    "create_vocab_file(yelp_vocabulary, \"Datasets/yelp-vocab.txt\")\n",
    "create_vocab_file(imdb_vocabulary, \"Datasets/IMDB-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From here on in, we will be actually training and testing\n",
    "\n",
    "# this method will take in a bunch of data and calculate F1\n",
    "def random_classifier(train_data, test_data, min_class, max_class):\n",
    "    test_prediction = []\n",
    "    train_prediction = []\n",
    "    \n",
    "    # go through data and choose a random class\n",
    "    for i in range(0, len(train_data)):\n",
    "        class_predict = random.randint(min_class, max_class)\n",
    "        train_prediction.append(class_predict)\n",
    "    for i in range(0, len(test_data)):\n",
    "        class_predict = random.randint(min_class, max_class)\n",
    "        test_prediction.append(class_predict)\n",
    "    return train_prediction, test_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.200285714286, Test F1 is 0.1965\n"
     ]
    }
   ],
   "source": [
    "#Yelp Random\n",
    "yelp_random_train_predict, yelp_random_test_predict = random_classifier(yelp_train, yelp_test, 1, 5)\n",
    "yelp_rand_train_f1 = f1_score(yelp_train['Evaluation'], yelp_random_train_predict, average='micro')\n",
    "yelp_rand_test_f1 = f1_score(yelp_test['Evaluation'], yelp_random_test_predict, average='micro')\n",
    "\n",
    "print(\"Train f1 is \" + str(yelp_rand_train_f1) + \", Test F1 is \" + str(yelp_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.499833477653, Test F1 is 0.497674418605\n"
     ]
    }
   ],
   "source": [
    "#IMDB Random\n",
    "\n",
    "imdb_random_train_predict, imdb_random_test_predict = random_classifier(imdb_train, imdb_test, 0,1)\n",
    "imdb_rand_train_f1 = f1_score(imdb_train['Evaluation'], imdb_random_train_predict)\n",
    "imdb_rand_test_f1 = f1_score(imdb_test['Evaluation'], imdb_random_test_predict)\n",
    "\n",
    "\n",
    "print(\"Train f1 is \" + str(imdb_rand_train_f1) + \", Test F1 is \" + str(imdb_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majority_classifier(train_data, test_data):\n",
    "    train_mode = train_data['Evaluation'].mode()[0]\n",
    "    test_mode = test_data['Evaluation'].mode()[0]\n",
    "    \n",
    "    train_prediction = [train_mode] * len(train_data)\n",
    "    test_prediction = [test_mode] * len(test_data)\n",
    "    return train_prediction, test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1 is 0.352571428571, Test F1 is 0.351\n"
     ]
    }
   ],
   "source": [
    "#Yelp Majority Classifier\n",
    "yelp_maj_train_predict, yelp_maj_test_predict = majority_classifier(yelp_train, yelp_test)\n",
    "yelp_rand_train_f1 = f1_score(yelp_train['Evaluation'], yelp_maj_train_predict, average='micro')\n",
    "yelp_rand_test_f1 = f1_score(yelp_test['Evaluation'], yelp_maj_test_predict, average='micro')\n",
    "\n",
    "print(\"Train f1 is \" + str(yelp_rand_train_f1) + \", Test F1 is \" + str(yelp_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 0.5, Test accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "#IMDB Majority Classifier\n",
    "imdb_maj_train_predict, imdb_maj_test_predict = majority_classifier(imdb_train, imdb_test)\n",
    "imdb_rand_train_f1 = accuracy_score(imdb_train['Evaluation'], imdb_maj_train_predict)\n",
    "imdb_rand_test_f1 = accuracy_score(imdb_test['Evaluation'], imdb_maj_test_predict)\n",
    "\n",
    "print(\"Train accuracy is \" + str(imdb_rand_train_f1) + \", Test accuracy is \" + str(imdb_rand_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will do naive bayes.\n",
    "yelp_vocabulary = get_vocabulary(yelp_train)\n",
    "yelp_words = [i[0] for i in yelp_vocabulary]\n",
    "\n",
    "#first make the training features matrix\n",
    "training_features_matrix = []\n",
    "training_target = []\n",
    "\n",
    "for sentence in yelp_train['Comment']:\n",
    "    training_features_matrix.append(generate_bbow_vector(yelp_words, sentence))\n",
    "\n",
    "for res in yelp_train['Evaluation']:\n",
    "    training_target.append(res)\n",
    "\n",
    "testing_features_matrix = []\n",
    "testing_target = []\n",
    "\n",
    "for sentence in yelp_test['Comment']:\n",
    "    testing_features_matrix.append(generate_bbow_vector(yelp_words, sentence))\n",
    "\n",
    "for res in yelp_test['Evaluation']:\n",
    "    testing_target.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Accuracy = 0.434285714286, Test F1 Accuarcy = 0.2265\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(training_features_matrix, training_target)\n",
    "train_target_pred = gnb.predict(training_features_matrix)\n",
    "test_target_pred = gnb.predict(testing_features_matrix)\n",
    "\n",
    "train_gnb_f1 = f1_score(yelp_train['Evaluation'], train_target_pred, average='micro')\n",
    "test_gnb_f1 = f1_score(yelp_test['Evaluation'], test_target_pred, average='micro')\n",
    "\n",
    "print(\"GaussianNB: Train F1 Accuracy = \" + str(train_gnb_f1) + \", Test F1 Accuarcy = \" + str(test_gnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees: Train F1 Accuracy = 0.999857142857, Test F1 Accuarcy = 0.3585\n"
     ]
    }
   ],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc.fit(training_features_matrix, training_target)\n",
    "train_target_pred = dtc.predict(training_features_matrix)\n",
    "test_target_pred = dtc.predict(testing_features_matrix)\n",
    "\n",
    "train_gnb_f1 = f1_score(yelp_train['Evaluation'], train_target_pred, average='micro')\n",
    "test_gnb_f1 = f1_score(yelp_test['Evaluation'], test_target_pred, average='micro')\n",
    "\n",
    "print(\"Decision Trees: Train F1 Accuracy = \" + str(train_gnb_f1) + \", Test F1 Accuarcy = \" + str(test_gnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmc = svm.SVC()\n",
    "svmc.fit(training_features_matrix, training_target)\n",
    "train_target_pred = svmc.predict(training_features_matrix)\n",
    "test_target_pred = svmc.predict(testing_features_matrix)\n",
    "\n",
    "train_gnb_f1 = f1_score(yelp_train['Evaluation'], train_target_pred, average='micro')\n",
    "test_gnb_f1 = f1_score(yelp_test['Evaluation'], test_target_pred, average='micro')\n",
    "\n",
    "print(\"Decision Trees: Train F1 Accuracy = \" + str(train_gnb_f1) + \", Test F1 Accuarcy = \" + str(test_gnb_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
